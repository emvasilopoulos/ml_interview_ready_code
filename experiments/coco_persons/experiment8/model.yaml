network:
  backbone:
    RGBCombinator:
      has_positional_encoding: false
      is_input_to_positional_encoder_normalized: false
      number_of_layers: 5
      d_model: 960 # Same as width of image
      n_heads: 16
      feed_forward_dimensions: 640 
      activation: gelu
      LayerNorm:
        has_layer_norm: true
        eps: 1e-06
        elementwise_affine: true
        bias: true
      mask_check: false
      combinator_activation: gelu
    VisionTransformerEncoder:
      has_positional_encoding: false
      is_input_to_positional_encoder_normalized: false
      number_of_layers: 0
      d_model: -1
      n_heads: -1
      feed_forward_dimensions: -1
      activation: gelu
      LayerNorm:
        has_layer_norm: true
        eps: 1e-06
        elementwise_affine: true
        bias: true
      mask_check: false
  head:
    VisionTransformerHead:
      has_positional_encoding: false
      is_input_to_positional_encoder_normalized: false
      number_of_layers: 3
      d_model: 640
      n_heads: 16
      feed_forward_dimensions: 960
      activation: gelu 
      LayerNorm:
        has_layer_norm: true
        eps: 1e-06
        elementwise_affine: false
        bias: false
      mask_check: false
