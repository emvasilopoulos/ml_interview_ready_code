network:
  backbone:
    RGBCombinator:
      has_positional_encoding: true
      is_input_to_positional_encoder_normalized: true
      number_of_layers: 5
      d_model: 1024 # Same as width of image
      n_heads: 32
      feed_forward_dimensions: 768 # Should match with feed forward dimensions of VisionTransformerEncoder for now
      activation: gelu
      LayerNorm:
        has_layer_norm: true
        eps: 1e-06
        elementwise_affine: true
        bias: true
      mask_check: false
      combinator_activation: sigmoid
    VisionTransformerEncoder:
      has_positional_encoding: false
      is_input_to_positional_encoder_normalized: false
      number_of_layers: 0
      d_model: -1
      n_heads: -1
      feed_forward_dimensions: -1
      activation: gelu
      LayerNorm:
        has_layer_norm: true
        eps: 1e-06
        elementwise_affine: true
        bias: true
      mask_check: false
  head:
    VisionTransformerHead:
      has_positional_encoding: false
      is_input_to_positional_encoder_normalized: false
      number_of_layers: 0
      d_model: -1
      n_heads: -1
      feed_forward_dimensions: -1
      activation: gelu 
      LayerNorm:
        has_layer_norm: false
        eps: 0
        elementwise_affine: false
        bias: false
      mask_check: false
