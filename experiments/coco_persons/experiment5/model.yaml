network:
  backbone:
    RGBCombinator:
      has_positional_encoding: true
      is_input_to_positional_encoder_normalized: true
      number_of_layers: 3
      d_model: 1024 # Same as width of image
      n_heads: 32
      feed_forward_dimensions: 768 # Should match with feed forward dimensions of VisionTransformerEncoder for now
      activation: gelu
      LayerNorm:
        has_layer_norm: true
        eps: 1e-06
        elementwise_affine: true
        bias: true
      mask_check: false
    VisionTransformerEncoder:
      has_positional_encoding: false
      is_input_to_positional_encoder_normalized: false
      number_of_layers: 3
      d_model: 1024
      n_heads: 32
      feed_forward_dimensions: 768
      activation: gelu
      LayerNorm:
        has_layer_norm: true
        eps: 1e-06
        elementwise_affine: true
        bias: true
      mask_check: false
  head:
    VisionTransformerHead:
      has_positional_encoding: false
      is_input_to_positional_encoder_normalized: false
      number_of_layers: 1
      d_model: 768
      n_heads: 32
      feed_forward_dimensions: 1024
      activation: sigmoid 
      LayerNorm:
        has_layer_norm: false
        eps: 0
        elementwise_affine: false
        bias: false
      mask_check: false
