{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pathlib\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "import mnn.vision.image_size\n",
    "import mnn.vision.models.vision_transformer.encoder.config as mnn_encoder_config\n",
    "import mnn.vision.config as mnn_config\n",
    "from mnn.vision.models.vision_transformer.e2e import MyVisionTransformer\n",
    "from mnn.vision.models.vision_transformer.tasks.object_detection import (\n",
    "    ObjectDetectionOrdinalHead,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. UTILITIES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_test(image: torch.Tensor, model: torch.nn.Module):\n",
    "    t0 = time.time()\n",
    "    output = model(image)\n",
    "    t1 = time.time()\n",
    "    print(\"Time taken:\", t1 - t0, \"seconds\")\n",
    "    print(\"Model's output shape:\", output.shape)\n",
    "    traced_model = torch.jit.trace(model.forward, image, check_trace=True, strict=True)\n",
    "    return traced_model\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def read_yaml_file(file_path: pathlib.Path) -> dict:\n",
    "    with file_path.open(mode=\"r\") as f:\n",
    "        # Python 3.11 need Loader\n",
    "        return yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. MODEL DEFINITION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" CONFIGURATION \"\"\"\n",
    "def load_model_config(yaml_path: pathlib.Path):\n",
    "    model_config_as_dict = read_yaml_file(yaml_path)\n",
    "    model_config = mnn_encoder_config.MyBackboneVitConfiguration.from_dict(\n",
    "        model_config_as_dict[\"network\"][\"backbone\"]\n",
    "    )\n",
    "    encoder_config = model_config.encoder_config\n",
    "    head_config = mnn_encoder_config.VisionTransformerEncoderConfiguration.from_dict(\n",
    "        model_config_as_dict[\"network\"][\"head\"][\"VisionTransformerHead\"]\n",
    "    )\n",
    "    return model_config, encoder_config, head_config\n",
    "\n",
    "def load_hyperparameters_config(yaml_path: pathlib.Path):\n",
    "    hyperparameters_config_as_dict = read_yaml_file(yaml_path)\n",
    "    hyperparameters_config = mnn_config.HyperparametersConfiguration.from_dict(hyperparameters_config_as_dict)\n",
    "    return hyperparameters_config\n",
    "\n",
    "\n",
    "model_config, encoder_config, head_config = load_model_config(pathlib.Path(\"model.yaml\"))\n",
    "hyperparameters_config = load_hyperparameters_config(pathlib.Path(\"hyperparameters.yaml\"))\n",
    "\n",
    "batch_size = hyperparameters_config.batch_size\n",
    "embedding_size = model_config.rgb_combinator_config.d_model\n",
    "sequence_length = model_config.rgb_combinator_config.feed_forward_dimensions\n",
    "image_size = mnn.vision.image_size.ImageSize(width=embedding_size, height=sequence_length)\n",
    "\n",
    "hidden_dim = embedding_size\n",
    "image_RGB = torch.rand(batch_size, 3, image_size.height, image_size.width) * 255\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 NETWORK DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VitObjectDetectionNetwork(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_config: mnn_encoder_config.MyBackboneVitConfiguration,\n",
    "        head_config: mnn_encoder_config.VisionTransformerEncoderConfiguration,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        expected_image_width = model_config.encoder_config.d_model\n",
    "        expected_image_height = model_config.encoder_config.feed_forward_dimensions\n",
    "        self.expected_image_size = mnn.vision.image_size.ImageSize(\n",
    "            width=expected_image_width, height=expected_image_height\n",
    "        )\n",
    "        self.encoder = MyVisionTransformer(model_config, image_size)\n",
    "        self.head = ObjectDetectionOrdinalHead(config=head_config)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.encoder(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "import mnn.vision.dataset.utilities\n",
    "import mnn.vision.models.heads.object_detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 MODEL UTILITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VitObjectDetectionNetwork(\n",
       "  (encoder): MyVisionTransformer(\n",
       "    (rgb_combinator): RGBCombinator(\n",
       "      (encoder): RawVisionTransformerRGBEncoder(\n",
       "        (multi_channels_encoder): ModuleList(\n",
       "          (0-2): 3 x RawVisionTransformerEncoder(\n",
       "            (positional_encoder): MyVisionPositionalEncoding()\n",
       "            (encoder_block): TransformerEncoder(\n",
       "              (layers): ModuleList(\n",
       "                (0): TransformerEncoderLayer(\n",
       "                  (self_attn): MultiheadAttention(\n",
       "                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "                  )\n",
       "                  (linear1): Linear(in_features=512, out_features=720, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (linear2): Linear(in_features=720, out_features=512, bias=True)\n",
       "                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "                  (dropout2): Dropout(p=0.1, inplace=False)\n",
       "                  (activation): GELU(approximate='none')\n",
       "                )\n",
       "              )\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (raw_vit): Sequential(\n",
       "              (0): MyVisionPositionalEncoding()\n",
       "              (1): TransformerEncoder(\n",
       "                (layers): ModuleList(\n",
       "                  (0): TransformerEncoderLayer(\n",
       "                    (self_attn): MultiheadAttention(\n",
       "                      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "                    )\n",
       "                    (linear1): Linear(in_features=512, out_features=720, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (linear2): Linear(in_features=720, out_features=512, bias=True)\n",
       "                    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "                    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "                    (activation): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (combinator): ThreeChannelsCombinator()\n",
       "    )\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-2): 3 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=720, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=720, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (head): ObjectDetectionOrdinalHead(\n",
       "    (layer): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_detection_model = VitObjectDetectionNetwork(\n",
    "    model_config=model_config,\n",
    "    head_config=head_config\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "object_detection_model.to(device=device, dtype=hyperparameters_config.floating_point_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnn.vision.dataset.coco.loader\n",
    "import pathlib\n",
    "\n",
    "dataset_dir = pathlib.Path(\"/home/emvasilopoulos/projects/ml_interview_ready_code/data/coco/\")\n",
    "\n",
    "train_dataset = mnn.vision.dataset.coco.loader.COCODatasetInstances2017(dataset_dir, \"train\", object_detection_model.expected_image_size)\n",
    "val_dataset = mnn.vision.dataset.coco.loader.COCODatasetInstances2017(dataset_dir, \"val\", object_detection_model.expected_image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(val_dataset, batch_size=hyperparameters_config.batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=hyperparameters_config.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, loss: 0.8032336831092834\n",
      "Step 10, loss: 0.7492992877960205\n",
      "Step 20, loss: 0.7297059297561646\n",
      "Step 30, loss: 0.7152820229530334\n",
      "Step 40, loss: 0.7098309993743896\n",
      "Step 50, loss: 0.7016305923461914\n",
      "Step 60, loss: 0.6968216300010681\n",
      "Step 70, loss: 0.6901659965515137\n",
      "Step 80, loss: 0.6797191500663757\n",
      "Step 90, loss: 0.6756447553634644\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 425 is out of bounds for dimension 0 with size 425",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(hyperparameters_config\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m---> 55\u001b[0m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_detection_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameters_config\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(object_detection_model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp1_object_detection.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m     val_once(val_loader, object_detection_model, loss_fn, hyperparameters_config)\n",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(train_loader, model, optimizer, loss_fn, hyperparameters_config)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_one_epoch\u001b[39m(\n\u001b[1;32m      2\u001b[0m     train_loader: torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader,\n\u001b[1;32m      3\u001b[0m     model: VitObjectDetectionNetwork,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     hyperparameters_config: mnn_config\u001b[38;5;241m.\u001b[39mHyperparametersConfiguration,\n\u001b[1;32m      7\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# important for batch normalization and dropout layers\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (image_batch, target0) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     10\u001b[0m         image_batch \u001b[38;5;241m=\u001b[39m image_batch\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m     11\u001b[0m             device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mhyperparameters_config\u001b[38;5;241m.\u001b[39mfloating_point_precision\n\u001b[1;32m     12\u001b[0m         )\n\u001b[1;32m     13\u001b[0m         target0 \u001b[38;5;241m=\u001b[39m target0\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m     14\u001b[0m             device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mhyperparameters_config\u001b[38;5;241m.\u001b[39mfloating_point_precision\n\u001b[1;32m     15\u001b[0m         )\n",
      "File \u001b[0;32m~/projects/ml_interview_ready_code/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/projects/ml_interview_ready_code/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/projects/ml_interview_ready_code/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/projects/ml_interview_ready_code/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/projects/ml_interview_ready_code/src/mnn/vision/dataset/coco/loader.py:128\u001b[0m, in \u001b[0;36mBaseCOCODatasetGrouped.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    124\u001b[0m img_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessor\u001b[38;5;241m.\u001b[39mpreprocess_image(\n\u001b[1;32m    125\u001b[0m     img_tensor, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpected_image_size, padding_percent\u001b[38;5;241m=\u001b[39mpadding_percent\n\u001b[1;32m    126\u001b[0m )\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Prepare output\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m output0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_percent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_percent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img_tensor, output0\n",
      "File \u001b[0;32m~/projects/ml_interview_ready_code/src/mnn/vision/dataset/coco/loader.py:152\u001b[0m, in \u001b[0;36mBaseCOCODatasetInstances.get_output\u001b[0;34m(self, img, annotations, padding_percent)\u001b[0m\n\u001b[1;32m    149\u001b[0m     bboxes\u001b[38;5;241m.\u001b[39mappend([x1 \u001b[38;5;241m/\u001b[39m img_w, y1 \u001b[38;5;241m/\u001b[39m img_h, w \u001b[38;5;241m/\u001b[39m img_w, h \u001b[38;5;241m/\u001b[39m img_h])\n\u001b[1;32m    150\u001b[0m     categories\u001b[38;5;241m.\u001b[39mappend(annotation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory_id\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 152\u001b[0m bboxes_as_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbbox_annotation_to_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbboxes\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_w\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m bboxes_as_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessor\u001b[38;5;241m.\u001b[39madjust_tensor_dimensions(\n\u001b[1;32m    156\u001b[0m     bboxes_as_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpected_image_size, padding_percent\u001b[38;5;241m=\u001b[39mpadding_percent\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# TODO - support categories somehow\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/ml_interview_ready_code/src/mnn/vision/dataset/object_detection/preprocessing.py:180\u001b[0m, in \u001b[0;36mObjectDetectionPreprocessing.bbox_annotation_to_mask\u001b[0;34m(y, mask_shape)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbbox_annotation_to_mask\u001b[39m(\n\u001b[1;32m    178\u001b[0m     y: torch\u001b[38;5;241m.\u001b[39mTensor, mask_shape: torch\u001b[38;5;241m.\u001b[39mSize\n\u001b[1;32m    179\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 180\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[43mmnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheads\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobject_detection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mObjectDetectionOrdinalTransformation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform_ground_truth_from_normalized_rectangles\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mask\n",
      "File \u001b[0;32m~/projects/ml_interview_ready_code/src/mnn/vision/models/heads/object_detection.py:220\u001b[0m, in \u001b[0;36mObjectDetectionOrdinalTransformation.transform_ground_truth_from_normalized_rectangles\u001b[0;34m(mask_shape, rectangles)\u001b[0m\n\u001b[1;32m    217\u001b[0m     mask_height \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(height_normalized \u001b[38;5;241m*\u001b[39m mask_shape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    218\u001b[0m     mask_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(width_normalized \u001b[38;5;241m*\u001b[39m mask_shape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 220\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[43mObjectDetectionOrdinalTransformation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_rectangle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_width\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mask\n",
      "File \u001b[0;32m~/projects/ml_interview_ready_code/src/mnn/vision/models/heads/object_detection.py:172\u001b[0m, in \u001b[0;36mObjectDetectionOrdinalTransformation.encode_rectangle\u001b[0;34m(mask, y, x, height, width)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_rectangle\u001b[39m(mask, y: \u001b[38;5;28mint\u001b[39m, x: \u001b[38;5;28mint\u001b[39m, height: \u001b[38;5;28mint\u001b[39m, width: \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    169\u001b[0m     mask \u001b[38;5;241m=\u001b[39m ObjectDetectionOrdinalTransformation\u001b[38;5;241m.\u001b[39mexpand_rectangle_inwards(\n\u001b[1;32m    170\u001b[0m         mask, y, x, height, width\n\u001b[1;32m    171\u001b[0m     )\n\u001b[0;32m--> 172\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[43mObjectDetectionOrdinalTransformation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_rectangle_outwards\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mask\n",
      "File \u001b[0;32m~/projects/ml_interview_ready_code/src/mnn/vision/models/heads/object_detection.py:80\u001b[0m, in \u001b[0;36mObjectDetectionOrdinalTransformation.expand_rectangle_outwards\u001b[0;34m(mask, y, x, height, width)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, expansion_size):\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# Top Left Corner\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;241m-\u001b[39m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m x \u001b[38;5;241m-\u001b[39m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     79\u001b[0m         mask[y \u001b[38;5;241m-\u001b[39m i \u001b[38;5;241m+\u001b[39m j, x \u001b[38;5;241m-\u001b[39m i] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(\n\u001b[0;32m---> 80\u001b[0m             \u001b[43mmask\u001b[49m\u001b[43m[\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m<\u001b[39m probability,\n\u001b[1;32m     81\u001b[0m             probability,\n\u001b[1;32m     82\u001b[0m             mask[y \u001b[38;5;241m-\u001b[39m i \u001b[38;5;241m+\u001b[39m j, x \u001b[38;5;241m-\u001b[39m i],\n\u001b[1;32m     83\u001b[0m         )\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;241m-\u001b[39m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m x \u001b[38;5;241m-\u001b[39m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     85\u001b[0m         mask[y \u001b[38;5;241m-\u001b[39m i, x \u001b[38;5;241m-\u001b[39m i \u001b[38;5;241m+\u001b[39m j] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(\n\u001b[1;32m     86\u001b[0m             mask[y \u001b[38;5;241m-\u001b[39m i, x \u001b[38;5;241m-\u001b[39m i \u001b[38;5;241m+\u001b[39m j] \u001b[38;5;241m<\u001b[39m probability,\n\u001b[1;32m     87\u001b[0m             probability,\n\u001b[1;32m     88\u001b[0m             mask[y \u001b[38;5;241m-\u001b[39m i, x \u001b[38;5;241m-\u001b[39m i \u001b[38;5;241m+\u001b[39m j],\n\u001b[1;32m     89\u001b[0m         )\n",
      "\u001b[0;31mIndexError\u001b[0m: index 425 is out of bounds for dimension 0 with size 425"
     ]
    }
   ],
   "source": [
    "def train_one_epoch(\n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    model: VitObjectDetectionNetwork,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    hyperparameters_config: mnn_config.HyperparametersConfiguration,\n",
    ") -> None:\n",
    "    model.train()  # important for batch normalization and dropout layers\n",
    "    for i, (image_batch, target0) in enumerate(train_loader):\n",
    "        image_batch = image_batch.to(\n",
    "            device=device, dtype=hyperparameters_config.floating_point_precision\n",
    "        )\n",
    "        target0 = target0.to(\n",
    "            device=device, dtype=hyperparameters_config.floating_point_precision\n",
    "        )\n",
    "        \n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(image_batch)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(output, target0)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Step {i}, loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "def val_once(val_loader, model, loss_fn, hyperparameters_config):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (image_batch, target0) in enumerate(val_loader):\n",
    "            image_batch = image_batch.to(\n",
    "                device=device, dtype=hyperparameters_config.floating_point_precision\n",
    "            )\n",
    "            target0 = target0.to(\n",
    "                device=device, dtype=hyperparameters_config.floating_point_precision\n",
    "            )\n",
    "            output = model(image_batch)\n",
    "            loss = loss_fn(output, target0)\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Validation step {i}, loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    object_detection_model.parameters(), lr=hyperparameters_config.learning_rate\n",
    ")\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "for epoch in range(hyperparameters_config.epochs):\n",
    "\n",
    "    train_one_epoch(\n",
    "        train_loader, object_detection_model, optimizer, loss_fn, hyperparameters_config\n",
    "    )\n",
    "    torch.save(object_detection_model.state_dict(), \"exp1_object_detection.pth\")\n",
    "    val_once(val_loader, object_detection_model, loss_fn, hyperparameters_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 INFERENCE PROFILING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.1274416446685791 seconds | image_shape: torch.Size([4, 3, 720, 512]) output_shape: torch.Size([4, 720, 512])\n",
      "Time taken: 0.04546070098876953 seconds | image_shape: torch.Size([4, 3, 720, 512]) output_shape: torch.Size([4, 720, 512])\n",
      "Time taken: 0.04407024383544922 seconds | image_shape: torch.Size([4, 3, 720, 512]) output_shape: torch.Size([4, 720, 512])\n",
      "Time taken: 0.04363131523132324 seconds | image_shape: torch.Size([4, 3, 720, 512]) output_shape: torch.Size([4, 720, 512])\n",
      "Time taken: 0.038485050201416016 seconds | image_shape: torch.Size([4, 3, 720, 512]) output_shape: torch.Size([4, 720, 512])\n",
      "Time taken: 0.035089731216430664 seconds | image_shape: torch.Size([4, 3, 720, 512]) output_shape: torch.Size([4, 720, 512])\n",
      "Time taken: 0.034650564193725586 seconds | image_shape: torch.Size([4, 3, 720, 512]) output_shape: torch.Size([4, 720, 512])\n",
      "Time taken: 0.03783130645751953 seconds | image_shape: torch.Size([4, 3, 720, 512]) output_shape: torch.Size([4, 720, 512])\n",
      "Time taken: 0.0355072021484375 seconds | image_shape: torch.Size([4, 3, 720, 512]) output_shape: torch.Size([4, 720, 512])\n",
      "Time taken: 0.03386497497558594 seconds | image_shape: torch.Size([4, 3, 720, 512]) output_shape: torch.Size([4, 720, 512])\n"
     ]
    }
   ],
   "source": [
    "import mnn.visualize\n",
    "import time\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(10):\n",
    "        image_size = object_detection_model.expected_image_size\n",
    "        image_RGB = torch.rand(hyperparameters_config.batch_size, 3, image_size.height, image_size.width) * 255\n",
    "        image_RGB = image_RGB.to(device=device, dtype=hyperparameters_config.floating_point_precision)\n",
    "        t0 = time.time()\n",
    "        output = object_detection_model(image_RGB)\n",
    "        out = output.detach().cpu().numpy()\n",
    "        t1 = time.time()\n",
    "        print(\"Time taken:\", t1 - t0, \"seconds | image_shape:\", image_RGB.shape, \"output_shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference         0.00%       0.000us         0.00%       0.000us       0.000us      39.239ms        55.23%      39.239ms      39.239ms             1  \n",
      "                                        model_inference        24.98%      10.571ms        88.90%      37.622ms      37.622ms       0.000us         0.00%      33.072ms      33.072ms             1  \n",
      "                     aten::scaled_dot_product_attention         1.00%     424.000us         3.04%       1.288ms     214.667us       0.000us         0.00%      15.283ms       2.547ms             6  \n",
      "          aten::_scaled_dot_product_efficient_attention         0.39%     163.000us         2.04%     864.000us     144.000us       0.000us         0.00%      15.283ms       2.547ms             6  \n",
      "                     aten::_efficient_attention_forward         0.98%     415.000us         1.56%     659.000us     109.833us      15.236ms        21.44%      15.283ms       2.547ms             6  \n",
      "fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEf...         0.00%       0.000us         0.00%       0.000us       0.000us      15.236ms        21.44%      15.236ms       2.539ms             6  \n",
      "                                           aten::linear         1.34%     565.000us        28.34%      11.994ms     499.750us       0.000us         0.00%      11.792ms     491.333us            24  \n",
      "                                 ampere_sgemm_128x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us      10.289ms        14.48%      10.289ms     428.708us            24  \n",
      "                                            aten::addmm         5.25%       2.223ms        12.75%       5.394ms     299.667us       5.939ms         8.36%       6.304ms     350.222us            18  \n",
      "                                           aten::matmul         0.36%     153.000us        11.95%       5.057ms     842.833us       0.000us         0.00%       4.658ms     776.333us             6  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 42.321ms\n",
      "Self CUDA time total: 71.048ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-09-29 18:44:05 6925:6925 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-09-29 18:44:05 6925:6925 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-09-29 18:44:05 6925:6925 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "with profile(activities=[\n",
    "        ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        object_detection_model(image_RGB)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 MODEL VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my_transformer.png'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mnn.visualize.make_dot(\n",
    "    output, params=dict(object_detection_model.named_parameters())\n",
    ").render(\"my_transformer\", format=\"png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
