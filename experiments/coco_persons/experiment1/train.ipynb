{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pathlib\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "import mnn.vision.image_size\n",
    "import mnn.vision.models.vision_transformer.encoder.config as mnn_encoder_config\n",
    "import mnn.vision.config as mnn_config\n",
    "from mnn.vision.models.vision_transformer.e2e import MyVisionTransformer\n",
    "from mnn.vision.models.vision_transformer.tasks.object_detection import (\n",
    "    ObjectDetectionOrdinalHead,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. UTILITIES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_test(image: torch.Tensor, model: torch.nn.Module):\n",
    "    t0 = time.time()\n",
    "    output = model(image)\n",
    "    t1 = time.time()\n",
    "    print(\"Time taken:\", t1 - t0, \"seconds\")\n",
    "    print(\"Model's output shape:\", output.shape)\n",
    "    traced_model = torch.jit.trace(model.forward, image, check_trace=True, strict=True)\n",
    "    return traced_model\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def read_yaml_file(file_path: pathlib.Path) -> dict:\n",
    "    with file_path.open(mode=\"r\") as f:\n",
    "        # Python 3.11 need Loader\n",
    "        return yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. MODEL DEFINITION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" CONFIGURATION \"\"\"\n",
    "def load_model_config(yaml_path: pathlib.Path):\n",
    "    model_config_as_dict = read_yaml_file(yaml_path)\n",
    "    model_config = mnn_encoder_config.MyBackboneVitConfiguration.from_dict(\n",
    "        model_config_as_dict[\"network\"][\"backbone\"]\n",
    "    )\n",
    "    encoder_config = model_config.encoder_config\n",
    "    head_config = mnn_encoder_config.VisionTransformerEncoderConfiguration.from_dict(\n",
    "        model_config_as_dict[\"network\"][\"head\"][\"VisionTransformerHead\"]\n",
    "    )\n",
    "    return model_config, encoder_config, head_config\n",
    "\n",
    "def load_hyperparameters_config(yaml_path: pathlib.Path):\n",
    "    hyperparameters_config_as_dict = read_yaml_file(yaml_path)\n",
    "    hyperparameters_config = mnn_config.HyperparametersConfiguration.from_dict(hyperparameters_config_as_dict)\n",
    "    return hyperparameters_config\n",
    "\n",
    "\n",
    "model_config, encoder_config, head_config = load_model_config(pathlib.Path(\"model.yaml\"))\n",
    "hyperparameters_config = load_hyperparameters_config(pathlib.Path(\"hyperparameters.yaml\"))\n",
    "\n",
    "batch_size = hyperparameters_config.batch_size\n",
    "embedding_size = model_config.rgb_combinator_config.d_model\n",
    "sequence_length = model_config.rgb_combinator_config.feed_forward_dimensions\n",
    "image_size = mnn.vision.image_size.ImageSize(width=embedding_size, height=sequence_length)\n",
    "\n",
    "hidden_dim = embedding_size\n",
    "image_RGB = torch.rand(batch_size, 3, image_size.height, image_size.width) * 255\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 NETWORK DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VitObjectDetectionNetwork(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_config: mnn_encoder_config.VisionTransformerEncoderConfiguration,\n",
    "        head_config: mnn_encoder_config.VisionTransformerEncoderConfiguration,\n",
    "        image_size: mnn.vision.image_size.ImageSize\n",
    "    ):\n",
    "        super().__init__()\n",
    "        expected_image_width = encoder_config.d_model\n",
    "        expected_image_height = -1\n",
    "        self.expected_image_size = (3, expected_image_height, expected_image_width)\n",
    "        self.encoder = MyVisionTransformer(encoder_config, image_size)\n",
    "        self.head = ObjectDetectionOrdinalHead(config=head_config)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.encoder(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "def preprocess_tensor(x: torch.Tensor, expected_image_width: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Expecting tensors of shape (3, H, W)\n",
    "    \"\"\"\n",
    "    x_w = x.shape[2]\n",
    "    if x_w != expected_image_width:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 MODEL UTILITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detection_model = VitObjectDetectionNetwork(\n",
    "    encoder_config=model_config,\n",
    "    head_config=head_config,\n",
    "    image_size=image_size,\n",
    "    dtype=hyperparameters_config.floating_point_precision,\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "object_detection_model.to(device=device, dtype=hyperparameters_config.floating_point_precision)\n",
    "image_RGB = image_RGB.to(device=device, dtype=hyperparameters_config.floating_point_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 INFERENCE PROFILING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.011376142501831055 seconds | image_shape: torch.Size([1, 3, 512, 512]) output_shape: torch.Size([1, 512, 512])\n",
      "Time taken: 0.009015083312988281 seconds | image_shape: torch.Size([1, 3, 512, 512]) output_shape: torch.Size([1, 512, 512])\n",
      "Time taken: 0.011157035827636719 seconds | image_shape: torch.Size([1, 3, 512, 512]) output_shape: torch.Size([1, 512, 512])\n",
      "Time taken: 0.009406089782714844 seconds | image_shape: torch.Size([1, 3, 512, 512]) output_shape: torch.Size([1, 512, 512])\n",
      "Time taken: 0.008602142333984375 seconds | image_shape: torch.Size([1, 3, 512, 512]) output_shape: torch.Size([1, 512, 512])\n",
      "Time taken: 0.011742830276489258 seconds | image_shape: torch.Size([1, 3, 512, 512]) output_shape: torch.Size([1, 512, 512])\n",
      "Time taken: 0.011185407638549805 seconds | image_shape: torch.Size([1, 3, 512, 512]) output_shape: torch.Size([1, 512, 512])\n",
      "Time taken: 0.00852823257446289 seconds | image_shape: torch.Size([1, 3, 512, 512]) output_shape: torch.Size([1, 512, 512])\n",
      "Time taken: 0.008677005767822266 seconds | image_shape: torch.Size([1, 3, 512, 512]) output_shape: torch.Size([1, 512, 512])\n",
      "Time taken: 0.009320735931396484 seconds | image_shape: torch.Size([1, 3, 512, 512]) output_shape: torch.Size([1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "import mnn.visualize\n",
    "import time\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(10):\n",
    "        image_RGB = torch.rand(hyperparameters_config.batch_size, 3, image_size.height, image_size.width) * 255\n",
    "        image_RGB = image_RGB.to(device=device, dtype=hyperparameters_config.floating_point_precision)\n",
    "        t0 = time.time()\n",
    "        output = object_detection_model(image_RGB)\n",
    "        out = output.detach().cpu().numpy()\n",
    "        t1 = time.time()\n",
    "        print(\"Time taken:\", t1 - t0, \"seconds | image_shape:\", image_RGB.shape, \"output_shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "with profile(activities=[\n",
    "        ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        object_detection_model(image_RGB)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 MODEL VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnn.visualize.make_dot(\n",
    "    output, params=dict(object_detection_model.named_parameters())\n",
    ").render(\"my_transformer\", format=\"png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
